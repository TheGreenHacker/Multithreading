#NAME: Jack Li
#EMAIL: jackli2014@gmail.com
#ID: 604754714

******************
*File Submissions*
******************
SortedList.h: supplied header file containing interfaces for linked list operations.

SortedList.c: C source module that implements linked list operations specified in header file.

lab2_list.c: driver program that implements specified command line options, creates one or more parallel threads to operate on shared linked list(s), and reports on the list and performance.

Makefile (targets):
default ... the lab2_list executable (compiling with the -Wall and -Wextra options).

tests ... run all specified test cases to generate CSV results

profile ... run tests with profiling tools to generate an execution profiling report

graphs ... use gnuplot to generate the required graphs

dist ... create the deliverable tarball

clean ... delete all programs and output generated by the Makefile

lab2b_list.csv: results for all test runs.

profile.out: execution profiling report showing where time was spent in the un-partitioned spin-lock implementation.

lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized list operations.

lab2b_2.png: mean time per mutex wait and mean time per operation for mutex-synchronized list operations.

lab2b_3.png: successful iterations vs. threads for each synchronization method.

lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists.

lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.

testlist.sh: bash script containing test cases to create data for .csv file


QUESTION 2.3.1 - CPU time in the basic list implementation:
In the 1 and 2-thread lists, most of the CPU time is spent on the list operation functions since with few threads, there is not so much contention among threads needing to wait for the lock to the critical section, which consists of all the list operations. Locks are readily available so synchronization is not much of an issue.

In the high-thread spin-lock tests, must of the CPU time is spent on threads spinning, waiting for the lock.

In the high-thread mutex tests, most of the CPU time is spent in the mutex functions, which although cheaper than just spinning, still is a source of overhead as they are costly operations. A higher number of threads also increases the contention among the threads as they all try to access the critical section.

QUESTION 2.3.2 - Execution Profiling:
The lines of code that are the most time consuming are the lines where the spin locks are acquired, particularly:

while (__sync_lock_test_and_set(&locks[hashval], 1))

This operation becomes more expensive with increasing number of threads since more threads are fighting for CPU time but the spin locks must protect the critical sections containing the linked list operations. Hence, threads not having the lock are forced to wait and spin, burning CPU cycles until they're finally able to get it.

QUESTION 2.3.3 - Mutex Wait Time:
The average lock-wait time for mutexes rises dramatically with more threads since the many threads are forced to wait for access to the critical sections. The contention of so many threads results in more threads in line waiting for the lock (the mutex in this case).

Completion time per operation rises less significantly with more threads since at least some thread is always guaranteed to make some progress (e.g. the thread holding the lock), and the line of waiting threads will get shorter after more threads have accessed the critical section.

The wait time per operation increases faster than the completion time per operation since the wait times for threads will tend to overlap with each other; hence increasing the average wait time.

QUESTION 2.3.4 - Performance of Partitioned Lists:
The more lists there are, the better the throughput (aka performance).

However, this trend will not continue without bounds. As some point each list element will have its own sublist, eliminating the need for threads to wait for another to access the critical section(s). 

The suggestion does not seem to be true since shortening the list reduces the times spent within the critical sections and also reduces contention. On the other hand, a single list with 1/N elements but with one lock will still suffer a similar problem to a big list in that multiple threads will be waiting on that one lock and forced to wait.
